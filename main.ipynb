{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at https://modelzoo.co/model/keras-realtime-multi-person-pose-estimation\n",
    "!git clone https://github.com/kevinlin311tw/keras-openpose-reproduce.git\n",
    "!mv keras-openpose-reproduce/* ./    &&    mkdir model/keras       \n",
    "!cd model   && sh get_keras_model.sh\n",
    "!pip3 -q install Cython scikit-image pandas zmq h5py opencv-python configobj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from config_reader import config_reader\n",
    "import scipy\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of model\n",
    "\n",
    "Helper functions to create a model.  These are the individual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def conv(x, nf, ks, name):\n",
    "    x1 = Conv2D(nf, (ks, ks), padding='same', name=name)(x)\n",
    "    return x1\n",
    "\n",
    "def pooling(x, ks, st, name):\n",
    "    x = MaxPooling2D((ks, ks), strides=(st, st), name=name)(x)\n",
    "    return x\n",
    "\n",
    "def vgg_block(x):\n",
    "     \n",
    "    # Block 1\n",
    "    x = conv(x, 64, 3, \"conv1_1\")\n",
    "    x = relu(x)\n",
    "    x = conv(x, 64, 3, \"conv1_2\")\n",
    "    x = relu(x)\n",
    "    x = pooling(x, 2, 2, \"pool1_1\")\n",
    "\n",
    "    # Block 2\n",
    "    x = conv(x, 128, 3, \"conv2_1\")\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv2_2\")\n",
    "    x = relu(x)\n",
    "    x = pooling(x, 2, 2, \"pool2_1\")\n",
    "    \n",
    "    # Block 3\n",
    "    x = conv(x, 256, 3, \"conv3_1\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 256, 3, \"conv3_2\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 256, 3, \"conv3_3\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 256, 3, \"conv3_4\")\n",
    "    x = relu(x)    \n",
    "    x = pooling(x, 2, 2, \"pool3_1\")\n",
    "    \n",
    "    # Block 4\n",
    "    x = conv(x, 512, 3, \"conv4_1\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 512, 3, \"conv4_2\")\n",
    "    x = relu(x)    \n",
    "    \n",
    "    # Additional non vgg layers\n",
    "    x = conv(x, 256, 3, \"conv4_3_CPM\")\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv4_4_CPM\")\n",
    "    x = relu(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def stage1_block(x, num_p, branch):\n",
    "    \n",
    "    # Block 1        \n",
    "    x = conv(x, 128, 3, \"conv5_1_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv5_2_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv5_3_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, 512, 1, \"conv5_4_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, num_p, 1, \"conv5_5_CPM_L%d\" % branch)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def stageT_block(x, num_p, stage, branch):\n",
    "        \n",
    "    # Block 1        \n",
    "    x = conv(x, 128, 7, \"Mconv1_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv2_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv3_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv4_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv5_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 1, \"Mconv6_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, num_p, 1, \"Mconv7_stage%d_L%d\" % (stage, branch))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keras model by assembling the blocks and load the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"model/keras/model.h5\" # orginal weights converted from caffe\n",
    "#weights_path = \"training/weights.best.h5\" # weights tarined from scratch \n",
    "\n",
    "input_shape = (None,None,3)\n",
    "\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "stages = 6\n",
    "np_branch1 = 38\n",
    "np_branch2 = 19\n",
    "\n",
    "img_normalized = Lambda(lambda x: x / 256 - 0.5)(img_input)  # [-0.5, 0.5]\n",
    "\n",
    "# VGG\n",
    "stage0_out = vgg_block(img_normalized)\n",
    "\n",
    "# stage 1\n",
    "stage1_branch1_out = stage1_block(stage0_out, np_branch1, 1)\n",
    "stage1_branch2_out = stage1_block(stage0_out, np_branch2, 2)\n",
    "x = Concatenate()([stage1_branch1_out, stage1_branch2_out, stage0_out])\n",
    "\n",
    "# stage t >= 2\n",
    "for sn in range(2, stages + 1):\n",
    "    stageT_branch1_out = stageT_block(x, np_branch1, sn, 1)\n",
    "    stageT_branch2_out = stageT_block(x, np_branch2, sn, 2)\n",
    "    if (sn < stages):\n",
    "        x = Concatenate()([stageT_branch1_out, stageT_branch2_out, stage0_out])\n",
    "\n",
    "model = Model(img_input, [stageT_branch1_out, stageT_branch2_out])\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model to an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "test_image_url = 'https://hips.hearstapps.com/hmg-prod/images/running-track-1667904802.jpg?crop=0.668xw:1.00xh;0.0737xw,0&resize=1200:*'\n",
    "response = urlopen(test_image_url)\n",
    "img_array = np.array(bytearray(response.read()), dtype=np.uint8)\n",
    "oriImg = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(oriImg[:, :, [2, 1, 0]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param, model_params = config_reader()\n",
    "\n",
    "multiplier = [x * model_params['boxsize'] / oriImg.shape[0] for x in param['scale_search']]\n",
    "print ('scale factors for this image:', multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs the model on the same image at different scales and aggregates the resulting heatmaps of body part locations (keypoints), and part affinities (PAF) (which encode the degree of association between keypoints).\n",
    "\n",
    "Then, shows sample heatmaps for right elbow and paf for right wrist and right elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n",
    "paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n",
    "# first figure shows padded images\n",
    "f, axarr = plt.subplots(1, len(multiplier))\n",
    "f.set_size_inches((20, 5))\n",
    "# second figure shows heatmaps\n",
    "f2, axarr2 = plt.subplots(1, len(multiplier))\n",
    "f2.set_size_inches((20, 5))\n",
    "# third figure shows PAFs\n",
    "f3, axarr3 = plt.subplots(2, len(multiplier))\n",
    "f3.set_size_inches((20, 10))\n",
    "\n",
    "for m in range(len(multiplier)):\n",
    "    scale = multiplier[m]\n",
    "    imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    imageToTest_padded, pad = util.padRightDownCorner(imageToTest, model_params['stride'], model_params['padValue'])        \n",
    "    axarr[m].imshow(imageToTest_padded[:,:,[2,1,0]])\n",
    "    axarr[m].set_title('Input image: scale %d' % m)\n",
    "\n",
    "    input_img = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,0,1,2)) # required shape (1, width, height, channels) \n",
    "    print(\"Input shape: \" + str(input_img.shape))  \n",
    "\n",
    "    output_blobs = model.predict(input_img)\n",
    "    print(\"Output shape (heatmap): \" + str(output_blobs[1].shape))\n",
    "    \n",
    "    # extract outputs, resize, and remove padding\n",
    "    heatmap = np.squeeze(output_blobs[1]) # output 1 is heatmaps\n",
    "    heatmap = cv2.resize(heatmap, (0,0), fx=model_params['stride'], fy=model_params['stride'], interpolation=cv2.INTER_CUBIC)\n",
    "    heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n",
    "    heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    paf = np.squeeze(output_blobs[0]) # output 0 is PAFs\n",
    "    paf = cv2.resize(paf, (0,0), fx=model_params['stride'], fy=model_params['stride'], interpolation=cv2.INTER_CUBIC)\n",
    "    paf = paf[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n",
    "    paf = cv2.resize(paf, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # visualization\n",
    "    axarr2[m].imshow(oriImg[:,:,[2,1,0]])\n",
    "    ax2 = axarr2[m].imshow(heatmap[:,:,3], alpha=.5) # right elbow\n",
    "    axarr2[m].set_title('Heatmaps (Relb): scale %d' % m)\n",
    "    \n",
    "    axarr3.flat[m].imshow(oriImg[:,:,[2,1,0]])\n",
    "    ax3x = axarr3.flat[m].imshow(paf[:,:,16], alpha=.5) # right elbow\n",
    "    axarr3.flat[m].set_title('PAFs (x comp. of Rwri to Relb): scale %d' % m)\n",
    "    axarr3.flat[len(multiplier) + m].imshow(oriImg[:,:,[2,1,0]])\n",
    "    ax3y = axarr3.flat[len(multiplier) + m].imshow(paf[:,:,17], alpha=.5) # right wrist\n",
    "    axarr3.flat[len(multiplier) + m].set_title('PAFs (y comp. of Relb to Rwri): scale %d' % m)\n",
    "    \n",
    "    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n",
    "    paf_avg = paf_avg + paf / len(multiplier)\n",
    "\n",
    "f2.subplots_adjust(right=0.93)\n",
    "cbar_ax = f2.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "_ = f2.colorbar(ax2, cax=cbar_ax)\n",
    "\n",
    "f3.subplots_adjust(right=0.93)\n",
    "cbar_axx = f3.add_axes([0.95, 0.57, 0.01, 0.3])\n",
    "_ = f3.colorbar(ax3x, cax=cbar_axx)\n",
    "cbar_axy = f3.add_axes([0.95, 0.15, 0.01, 0.3])\n",
    "_ = f3.colorbar(ax3y, cax=cbar_axy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize keypoint heat maps and pafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Display the original image\n",
    "plt.imshow(oriImg[:, :, [2, 1, 0]])\n",
    "\n",
    "# Display the heatmap on top of the original image with alpha blending\n",
    "plt.imshow(heatmap_avg[:, :, 9], alpha=0.5)\n",
    "\n",
    "# Set the figure size and adjust the subplot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 12)\n",
    "fig.subplots_adjust(right=0.93)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "cbar = fig.colorbar(plt.imshow(np.zeros((10, 10))), cax=cbar_ax)  # Assuming a dummy image for colorbar\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import ma\n",
    "U = paf_avg[:,:,16] * -1\n",
    "V = paf_avg[:,:,17]\n",
    "X, Y = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))\n",
    "M = np.zeros(U.shape, dtype='bool')\n",
    "M[U**2 + V**2 < 0.5 * 0.5] = True\n",
    "U = ma.masked_array(U, mask=M)\n",
    "V = ma.masked_array(V, mask=M)\n",
    "\n",
    "# 1\n",
    "plt.figure()\n",
    "plt.imshow(oriImg[:,:,[2,1,0]], alpha = .5)\n",
    "s = 5\n",
    "Q = plt.quiver(X[::s,::s], Y[::s,::s], U[::s,::s], V[::s,::s], \n",
    "               scale=50, headaxislength=4, alpha=.5, width=0.001, color='r')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect the body parts (keypoints) in the heat maps and visualize them\n",
    "\n",
    "Visualise all detected body parts. Note that we use peaks in heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "all_peaks = []\n",
    "peak_counter = 0\n",
    "\n",
    "for part in range(19-1):\n",
    "    map_ori = heatmap_avg[:,:,part]\n",
    "    map = gaussian_filter(map_ori, sigma=3)\n",
    "    \n",
    "    map_left = np.zeros(map.shape)\n",
    "    map_left[1:,:] = map[:-1,:]\n",
    "    map_right = np.zeros(map.shape)\n",
    "    map_right[:-1,:] = map[1:,:]\n",
    "    map_up = np.zeros(map.shape)\n",
    "    map_up[:,1:] = map[:,:-1]\n",
    "    map_down = np.zeros(map.shape)\n",
    "    map_down[:,:-1] = map[:,1:]\n",
    "    \n",
    "    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param['thre1']))\n",
    "    peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])) # note reverse\n",
    "    peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n",
    "    id = range(peak_counter, peak_counter + len(peaks))\n",
    "    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n",
    "\n",
    "    all_peaks.append(peaks_with_score_and_id)\n",
    "    peak_counter += len(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# visualize\n",
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
    "cmap = plt.cm.get_cmap('hsv')\n",
    "\n",
    "# Assuming oriImg and canvas are successfully loaded\n",
    "if oriImg is not None and canvas is not None:\n",
    "    canvas_resized = cv2.resize(canvas, (oriImg.shape[1], oriImg.shape[0]))\n",
    "\n",
    "    for i in range(18):\n",
    "        rgba = np.array(cmap(1 - i/18. - 1./36))\n",
    "        rgba[0:3] *= 255\n",
    "        for j in range(len(all_peaks[i])):\n",
    "            cv2.circle(canvas_resized, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n",
    "\n",
    "    to_plot = cv2.addWeighted(oriImg, 0.3, canvas_resized, 0.7, 0)\n",
    "\n",
    "    plt.imshow(to_plot[:, :, [2, 1, 0]])\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 12)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Failed to load one of the images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link the body parts and visualize them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find connection in the specified sequence, center 29 is in the position 15\n",
    "limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n",
    "           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n",
    "           [1,16], [16,18], [3,17], [6,18]]\n",
    "# the middle joints heatmap correpondence\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n",
    "          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n",
    "          [55,56], [37,38], [45,46]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_all = []\n",
    "special_k = []\n",
    "mid_num = 10\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n",
    "    candA = all_peaks[limbSeq[k][0]-1]\n",
    "    candB = all_peaks[limbSeq[k][1]-1]\n",
    "    nA = len(candA)\n",
    "    nB = len(candB)\n",
    "    indexA, indexB = limbSeq[k]\n",
    "    if(nA != 0 and nB != 0):\n",
    "        connection_candidate = []\n",
    "        for i in range(nA):\n",
    "            for j in range(nB):\n",
    "                vec = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n",
    "                # failure case when 2 body parts overlaps\n",
    "                if norm == 0:\n",
    "                    continue\n",
    "                vec = np.divide(vec, norm)\n",
    "                \n",
    "                startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n",
    "                               np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n",
    "                \n",
    "                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n",
    "                                  for I in range(len(startend))])\n",
    "                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n",
    "                                  for I in range(len(startend))])\n",
    "\n",
    "                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n",
    "                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n",
    "                criterion1 = len(np.nonzero(score_midpts > param['thre2'])[0]) > 0.8 * len(score_midpts)\n",
    "                criterion2 = score_with_dist_prior > 0\n",
    "                if criterion1 and criterion2:\n",
    "                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n",
    "\n",
    "        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n",
    "        connection = np.zeros((0,5))\n",
    "        for c in range(len(connection_candidate)):\n",
    "            i,j,s = connection_candidate[c][0:3]\n",
    "            if(i not in connection[:,3] and j not in connection[:,4]):\n",
    "                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n",
    "                if(len(connection) >= min(nA, nB)):\n",
    "                    break\n",
    "\n",
    "        connection_all.append(connection)\n",
    "    else:\n",
    "        special_k.append(k)\n",
    "        connection_all.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last number in each row is the total parts number of that person\n",
    "# the second last number in each row is the score of the overall configuration\n",
    "subset = -1 * np.ones((0, 20))\n",
    "candidate = np.array([item for sublist in all_peaks for item in sublist])\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    if k not in special_k:\n",
    "        partAs = connection_all[k][:,0]\n",
    "        partBs = connection_all[k][:,1]\n",
    "        indexA, indexB = np.array(limbSeq[k]) - 1\n",
    "\n",
    "        for i in range(len(connection_all[k])): #= 1:size(temp,1)\n",
    "            found = 0\n",
    "            subset_idx = [-1, -1]\n",
    "            for j in range(len(subset)): #1:size(subset,1):\n",
    "                if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n",
    "                    subset_idx[found] = j\n",
    "                    found += 1\n",
    "            \n",
    "            if found == 1:\n",
    "                j = subset_idx[0]\n",
    "                if(subset[j][indexB] != partBs[i]):\n",
    "                    subset[j][indexB] = partBs[i]\n",
    "                    subset[j][-1] += 1\n",
    "                    subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "            elif found == 2: # if found 2 and disjoint, merge them\n",
    "                j1, j2 = subset_idx\n",
    "                print (\"found = 2\")\n",
    "                membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n",
    "                if len(np.nonzero(membership == 2)[0]) == 0: #merge\n",
    "                    subset[j1][:-2] += (subset[j2][:-2] + 1)\n",
    "                    subset[j1][-2:] += subset[j2][-2:]\n",
    "                    subset[j1][-2] += connection_all[k][i][2]\n",
    "                    subset = np.delete(subset, j2, 0)\n",
    "                else: # as like found == 1\n",
    "                    subset[j1][indexB] = partBs[i]\n",
    "                    subset[j1][-1] += 1\n",
    "                    subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "\n",
    "            # if find no partA in the subset, create a new subset\n",
    "            elif not found and k < 17:\n",
    "                row = -1 * np.ones(20)\n",
    "                row[indexA] = partAs[i]\n",
    "                row[indexB] = partBs[i]\n",
    "                row[-1] = 2\n",
    "                row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n",
    "                subset = np.vstack([subset, row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete some rows of subset which has few parts occur\n",
    "deleteIdx = [];\n",
    "for i in range(len(subset)):\n",
    "    if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n",
    "        deleteIdx.append(i)\n",
    "subset = np.delete(subset, deleteIdx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize linked body parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize canvas\n",
    "canvas = np.zeros_like(oriImg)  # Assuming oriImg is the original image\n",
    "\n",
    "# Check if canvas is successfully created\n",
    "if canvas is not None:\n",
    "    stickwidth = 4\n",
    "\n",
    "    for i in range(17):\n",
    "        for n in range(len(subset)):\n",
    "            index = subset[n][np.array(limbSeq[i])-1]\n",
    "            if -1 in index:\n",
    "                continue\n",
    "            cur_canvas = canvas.copy()\n",
    "            Y = candidate[index.astype(int), 0]\n",
    "            X = candidate[index.astype(int), 1]\n",
    "            mX = np.mean(X)\n",
    "            mY = np.mean(Y)\n",
    "            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
    "            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
    "            polygon = cv2.ellipse2Poly((int(mY), int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n",
    "            cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n",
    "            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
    "\n",
    "    plt.imshow(canvas[:, :, [2, 1, 0]])\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 12)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Failed to create canvas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
